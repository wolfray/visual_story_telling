# Visual Story Telling

<p align="justify"> 
With prodigious improvement in the field of artificial intelligence, researchers are now moving towards building agents capable of mimicking human-like understanding of visual storytelling. Classifying and categorizing images as well as recognizing the physical objects contained within is a well established capability of deep neural networks which has been employed in many practical applications. But moving from static to sequential vision and describing the sequences in language is more attuned to human-like understanding of the events in the sequences is a complex  and challenging task. In this project, we want to explore various architectures to 1) model the interdependency of each component in the sequence to generate descriptions for new sequences in a <b>seq2seq</b> fashion and 2) model the story-like nature of the captions and test the model's ability to summarize the entire sequence.
<p align="justify">

<p align="justify">
Proposed model as sequential captioning is shown below.
<p align="justify">

<p align="center">
<img src="http://i.imgur.com/vnTpRc3.png">
<br><br> Figure: Attentive sequence-to-sequence architecture to learn sequential vision to language
<p align="center">

#### Direct Link: [Project Proposal](https://drive.google.com/file/d/0B8ZGlkqDw7hFTjlYOVVCcFMtLTQ/view?usp=sharing)
